{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16a8c061-1eee-4db9-a5f9-c9f9c7760aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import udf, pandas_udf\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07a8fb69-5e2c-49b9-81b3-5d2e4b89642a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/19 23:53:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/03/19 23:53:31 WARN StandaloneSchedulerBackend: Dynamic allocation enabled without spark.executor.cores explicitly set, you may get more executors allocated than expected. It's recommended to set spark.executor.cores explicitly. Please check SPARK-30299 for more details.\n"
     ]
    }
   ],
   "source": [
    "spark_session = SparkSession.builder \\\n",
    "    .master(\"spark://192.168.2.47:7077\") \\\n",
    "    .appName(\"Group40_Project\") \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", True) \\\n",
    "    .config(\"spark.dynamicAllocation.shuffleTracking.enabled\", True) \\\n",
    "    .config(\"spark.shuffle.service.enabled\", False) \\\n",
    "    .config(\"spark.dynamicAllocation.executorIdleTimeout\", \"30s\") \\\n",
    "    .config(\"spark.executor.memory\", \"5G\") \\\n",
    "    .config(\"spark.cores.max\", 12) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# RDD API\n",
    "spark_context = spark_session.sparkContext\n",
    "spark_context.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51741c95-afd2-4f5b-bba1-9e2cfdda0a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Loading the dataset without the corrupted values\n",
    "df = spark_session.read.option(\"mode\", \"DROPMALFORMED\").json(\"hdfs://192.168.2.47:9000/data-project/reddit_50k.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e7e7730-627f-4fd1-9aa3-180db13e8613",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters and extra spaces\n",
    "    text = re.sub(r'\\W+', ' ', text)\n",
    "    \n",
    "    # Split text into words\n",
    "    words = text.split()\n",
    "\n",
    "    # Remove stopwords\n",
    "    words = [w for w in words if w not in stop_words]\n",
    "\n",
    "    # Lemmatize words\n",
    "    words = [lemmatizer.lemmatize(w) for w in words]\n",
    "\n",
    "    out_text = ' '.join(words)\n",
    "\n",
    "    return out_text\n",
    "\n",
    "def sentiment_score(text):\n",
    "    return sia.polarity_scores(text)['compound']\n",
    "\n",
    "def sentiment_label(score):\n",
    "    if score > 0.05:\n",
    "        return 'positive'\n",
    "    elif score < -0.05:\n",
    "        return 'negative'\n",
    "    else:\n",
    "        return 'neutral'\n",
    "\n",
    "preprocess_text_udf = udf(preprocess_text, \"string\")\n",
    "sentiment_score_udf = udf(sentiment_score, \"float\")\n",
    "sentiment_label_udf = udf(sentiment_label, \"string\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac8728d8-b82c-488b-94f3-26d1ba342d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "@pandas_udf(\"string\")\n",
    "def preprocess_text_udf(text_series: pd.Series) -> pd.Series:\n",
    "    def preprocess(text):\n",
    "        # Lowercase the text\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove special characters and extra spaces\n",
    "        text = re.sub(r'\\W+', ' ', text)\n",
    "        \n",
    "        # Split text into words\n",
    "        words = text.split()\n",
    "    \n",
    "        # Remove stopwords\n",
    "        words = [w for w in words if w not in stop_words]\n",
    "    \n",
    "        # Lemmatize words\n",
    "        words = [lemmatizer.lemmatize(w) for w in words]\n",
    "    \n",
    "        out_text = ' '.join(words)\n",
    "    \n",
    "        return out_text\n",
    "\n",
    "    return text_series.apply(preprocess)\n",
    "\n",
    "@pandas_udf(\"float\")\n",
    "def sentiment_score_udf(text_series: pd.Series) -> pd.Series:\n",
    "    return text_series.apply(lambda text: sia.polarity_scores(text)['compound'])\n",
    "\n",
    "@pandas_udf(\"string\")\n",
    "def sentiment_label_udf(score_series: pd.Series) -> pd.Series:\n",
    "    def label(score):\n",
    "        if score > 0.05:\n",
    "            return 'positive'\n",
    "        elif score < -0.05:\n",
    "            return 'negative'\n",
    "        else:\n",
    "            return 'neutral'\n",
    "    return score_series.apply(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e6bc83d-c1ea-40e8-a38f-28ed117dedfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df_prep = df.drop('body', 'content', 'id', 'subreddit_id', 'title', 'author', 'content_len', 'summary', 'summary_len')\n",
    "\n",
    "k = 25 # number of subreddits to consider\n",
    "df_top = df_prep.select('subreddit') \\\n",
    "                .groupBy('subreddit').count() \\\n",
    "                .sort('count', ascending=False) \\\n",
    "                .limit(k)\n",
    "\n",
    "topk_subreddits = [df_top.collect()[i][0] for i in range(k)]\n",
    "\n",
    "df_topk = df_prep.filter(df_prep['subreddit'].isin(topk_subreddits))\n",
    "\n",
    "df_topk = df_topk.withColumn(\"clean_text\", preprocess_text_udf(df_topk['normalizedBody']))\n",
    "\n",
    "df_vader = df_topk.withColumn(\"sentiment_score\", sentiment_score_udf(df_topk['clean_text']))\n",
    "df_vader = df_vader.withColumn(\"sentiment_label\", sentiment_label_udf(df_vader['sentiment_score']))\n",
    "\n",
    "df_avg = df_vader.groupBy(\"subreddit\") \\\n",
    "                 .agg(F.round(F.avg(\"sentiment_score\"), 4).alias(\"avg_sentiment_score\")) \\\n",
    "                 .limit(k)\n",
    "\n",
    "df_avg = df_avg.withColumn(\"sentiment_label\", sentiment_label_udf(df_avg['avg_sentiment_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f0b28aae-107e-4492-ab0d-79a2f344fbd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 413:============================>                            (1 + 1) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+---------------+\n",
      "|          subreddit|avg_sentiment_score|sentiment_label|\n",
      "+-------------------+-------------------+---------------+\n",
      "|         offmychest|             0.2412|       positive|\n",
      "|          AskReddit|             0.1281|       positive|\n",
      "|             videos|             0.0718|       positive|\n",
      "|              DotA2|             0.3839|       positive|\n",
      "|      todayilearned|             0.1156|       positive|\n",
      "|      AdviceAnimals|              0.074|       positive|\n",
      "|     DestinyTheGame|              0.377|       positive|\n",
      "|      relationships|             0.4971|       positive|\n",
      "|               pics|             0.0994|       positive|\n",
      "|            Fitness|             0.3721|       positive|\n",
      "|         reddit.com|              0.242|       positive|\n",
      "|          worldnews|            -0.1374|       negative|\n",
      "|    TwoXChromosomes|             0.3153|       positive|\n",
      "|           politics|             0.0821|       positive|\n",
      "|               IAmA|             0.2157|       positive|\n",
      "|              trees|             0.3952|       positive|\n",
      "|relationship_advice|             0.5735|       positive|\n",
      "|    leagueoflegends|             0.3298|       positive|\n",
      "|  explainlikeimfive|             0.2446|       positive|\n",
      "|                WTF|            -0.0267|        neutral|\n",
      "|            atheism|             0.2459|       positive|\n",
      "|             gaming|             0.3748|       positive|\n",
      "|               tifu|             0.1153|       positive|\n",
      "|                sex|             0.5439|       positive|\n",
      "|              funny|             0.0727|       positive|\n",
      "+-------------------+-------------------+---------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df_avg.show(k)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
