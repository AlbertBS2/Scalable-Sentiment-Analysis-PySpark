{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "16a8c061-1eee-4db9-a5f9-c9f9c7760aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.functions import udf, pandas_udf\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "from time import perf_counter\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07a8fb69-5e2c-49b9-81b3-5d2e4b89642a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/20 01:48:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/03/20 01:48:28 WARN StandaloneSchedulerBackend: Dynamic allocation enabled without spark.executor.cores explicitly set, you may get more executors allocated than expected. It's recommended to set spark.executor.cores explicitly. Please check SPARK-30299 for more details.\n"
     ]
    }
   ],
   "source": [
    "spark_session = SparkSession.builder \\\n",
    "    .master(\"spark://192.168.2.47:7077\") \\\n",
    "    .appName(\"manual_test\") \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", True) \\\n",
    "    .config(\"spark.dynamicAllocation.shuffleTracking.enabled\", True) \\\n",
    "    .config(\"spark.shuffle.service.enabled\", False) \\\n",
    "    .config(\"spark.dynamicAllocation.executorIdleTimeout\", \"30s\") \\\n",
    "    .config(\"spark.executor.instances\", 1) \\\n",
    "    .config(\"spark.executor.memory\", \"5G\") \\\n",
    "    .config(\"spark.cores.max\", 12) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# RDD API\n",
    "spark_context = spark_session.sparkContext\n",
    "spark_context.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51741c95-afd2-4f5b-bba1-9e2cfdda0a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Loading the dataset without the corrupted values\n",
    "df = spark_session.read.option(\"mode\", \"DROPMALFORMED\").json(\"hdfs://192.168.2.47:9000/data-project/reddit_50k.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac8728d8-b82c-488b-94f3-26d1ba342d26",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "@pandas_udf(\"string\")\n",
    "def preprocess_text_udf(text_series: pd.Series) -> pd.Series:\n",
    "    def preprocess(text):\n",
    "        # Lowercase the text\n",
    "        text = text.lower()\n",
    "        \n",
    "        # Remove special characters and extra spaces\n",
    "        text = re.sub(r'\\W+', ' ', text)\n",
    "        \n",
    "        # Split text into words\n",
    "        words = text.split()\n",
    "    \n",
    "        # Remove stopwords\n",
    "        words = [w for w in words if w not in stop_words]\n",
    "    \n",
    "        # Lemmatize words\n",
    "        words = [lemmatizer.lemmatize(w) for w in words]\n",
    "    \n",
    "        out_text = ' '.join(words)\n",
    "    \n",
    "        return out_text\n",
    "\n",
    "    return text_series.apply(preprocess)\n",
    "\n",
    "@pandas_udf(\"float\")\n",
    "def sentiment_score_udf(text_series: pd.Series) -> pd.Series:\n",
    "    return text_series.apply(lambda text: sia.polarity_scores(text)['compound'])\n",
    "\n",
    "@pandas_udf(\"string\")\n",
    "def sentiment_label_udf(score_series: pd.Series) -> pd.Series:\n",
    "    def label(score):\n",
    "        if score > 0.05:\n",
    "            return 'positive'\n",
    "        elif score < -0.05:\n",
    "            return 'negative'\n",
    "        else:\n",
    "            return 'neutral'\n",
    "    return score_series.apply(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4e6bc83d-c1ea-40e8-a38f-28ed117dedfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread \"serve-DataFrame\" java.net.SocketTimeoutException: Accept timed out\n",
      "\tat java.net.PlainSocketImpl.socketAccept(Native Method)\n",
      "\tat java.net.AbstractPlainSocketImpl.accept(AbstractPlainSocketImpl.java:409)\n",
      "\tat java.net.ServerSocket.implAccept(ServerSocket.java:560)\n",
      "\tat java.net.ServerSocket.accept(ServerSocket.java:528)\n",
      "\tat org.apache.spark.security.SocketAuthServer$$anon$1.run(SocketAuthServer.scala:65)\n",
      "[Stage 56:===========================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+---------------+\n",
      "|          subreddit|avg_sentiment_score|sentiment_label|\n",
      "+-------------------+-------------------+---------------+\n",
      "|         offmychest|             0.2412|       positive|\n",
      "|          AskReddit|             0.1281|       positive|\n",
      "|             videos|             0.0718|       positive|\n",
      "|              DotA2|             0.3839|       positive|\n",
      "|      todayilearned|             0.1156|       positive|\n",
      "|      AdviceAnimals|              0.074|       positive|\n",
      "|     DestinyTheGame|              0.377|       positive|\n",
      "|      relationships|             0.4971|       positive|\n",
      "|               pics|             0.0994|       positive|\n",
      "|            Fitness|             0.3721|       positive|\n",
      "|         reddit.com|              0.242|       positive|\n",
      "|          worldnews|            -0.1374|       negative|\n",
      "|    TwoXChromosomes|             0.3153|       positive|\n",
      "|           politics|             0.0821|       positive|\n",
      "|               IAmA|             0.2157|       positive|\n",
      "|              trees|             0.3952|       positive|\n",
      "|relationship_advice|             0.5735|       positive|\n",
      "|    leagueoflegends|             0.3298|       positive|\n",
      "|  explainlikeimfive|             0.2446|       positive|\n",
      "|                WTF|            -0.0267|        neutral|\n",
      "|            atheism|             0.2459|       positive|\n",
      "|             gaming|             0.3748|       positive|\n",
      "|               tifu|             0.1153|       positive|\n",
      "|                sex|             0.5439|       positive|\n",
      "|              funny|             0.0727|       positive|\n",
      "+-------------------+-------------------+---------------+\n",
      "\n",
      "42.952861504018074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "init_time = perf_counter()\n",
    "\n",
    "df_prep = df.drop('body', 'content', 'id', 'subreddit_id', 'title', 'author', 'content_len', 'summary', 'summary_len')\n",
    "\n",
    "k = 25 # number of subreddits to consider\n",
    "df_top = df_prep.select('subreddit') \\\n",
    "                .groupBy('subreddit').count() \\\n",
    "                .sort('count', ascending=False) \\\n",
    "                .limit(k)\n",
    "\n",
    "topk_subreddits = [df_top.collect()[i][0] for i in range(k)]\n",
    "\n",
    "df_topk = df_prep.filter(df_prep['subreddit'].isin(topk_subreddits))\n",
    "\n",
    "df_topk = df_topk.withColumn(\"clean_text\", preprocess_text_udf(df_topk['normalizedBody']))\n",
    "\n",
    "df_vader = df_topk.withColumn(\"sentiment_score\", sentiment_score_udf(df_topk['clean_text']))\n",
    "df_vader = df_vader.withColumn(\"sentiment_label\", sentiment_label_udf(df_vader['sentiment_score']))\n",
    "\n",
    "df_avg = df_vader.groupBy(\"subreddit\") \\\n",
    "                 .agg(F.round(F.avg(\"sentiment_score\"), 4).alias(\"avg_sentiment_score\")) \\\n",
    "                 .limit(k)\n",
    "\n",
    "df_avg = df_avg.withColumn(\"sentiment_label\", sentiment_label_udf(df_avg['avg_sentiment_score']))\n",
    "\n",
    "df_avg.show(k)\n",
    "\n",
    "end_time = perf_counter()\n",
    "\n",
    "elapsed_time = end_time - init_time\n",
    "print(elapsed_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9b2d504a-a505-4e3d-8f40-52a0bf305953",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_session.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970e5526-1699-449d-8103-8369bd5332ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a767046-ba42-4d85-ad0e-9815f22f81d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "992a5921-a955-4bbc-ba44-39876519204b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_session = SparkSession.builder \\\n",
    "    .master(\"spark://192.168.2.47:7077\") \\\n",
    "    .appName(\"manual_test\") \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", True) \\\n",
    "    .config(\"spark.dynamicAllocation.shuffleTracking.enabled\", True) \\\n",
    "    .config(\"spark.shuffle.service.enabled\", False) \\\n",
    "    .config(\"spark.dynamicAllocation.executorIdleTimeout\", \"30s\") \\\n",
    "    .config(\"spark.executor.instances\", 2) \\\n",
    "    .config(\"spark.executor.memory\", \"5G\") \\\n",
    "    .config(\"spark.cores.max\", 12) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# RDD API\n",
    "spark_context = spark_session.sparkContext\n",
    "spark_context.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5f5c71fc-bc11-4929-916d-015056da6a16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df = spark_session.read.option(\"mode\", \"DROPMALFORMED\").json(\"hdfs://192.168.2.47:9000/data-project/reddit_50k.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "62353740-e2bd-46fe-834b-60d1e89ff04b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/20 01:57:36 ERROR DAGScheduler: Failed to update accumulator 0 (org.apache.spark.api.python.PythonAccumulatorV2) for task 3\n",
      "org.apache.spark.SparkException: EOF reached before Python server acknowledged\n",
      "\tat org.apache.spark.api.python.PythonAccumulatorV2.merge(PythonRDD.scala:751)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$updateAccumulators$1(DAGScheduler.scala:1694)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$updateAccumulators$1$adapted(DAGScheduler.scala:1685)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.updateAccumulators(DAGScheduler.scala:1685)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1838)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3054)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "25/03/20 01:57:38 ERROR DAGScheduler: Failed to update accumulator 0 (org.apache.spark.api.python.PythonAccumulatorV2) for task 2\n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:111)\n",
      "\tat java.net.SocketOutputStream.write(SocketOutputStream.java:155)\n",
      "\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n",
      "\tat java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)\n",
      "\tat java.io.DataOutputStream.flush(DataOutputStream.java:123)\n",
      "\tat org.apache.spark.api.python.PythonAccumulatorV2.merge(PythonRDD.scala:747)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$updateAccumulators$1(DAGScheduler.scala:1694)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$updateAccumulators$1$adapted(DAGScheduler.scala:1685)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.updateAccumulators(DAGScheduler.scala:1685)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1838)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3054)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "25/03/20 01:57:39 ERROR DAGScheduler: Failed to update accumulator 0 (org.apache.spark.api.python.PythonAccumulatorV2) for task 1\n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:111)\n",
      "\tat java.net.SocketOutputStream.write(SocketOutputStream.java:155)\n",
      "\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n",
      "\tat java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)\n",
      "\tat java.io.DataOutputStream.flush(DataOutputStream.java:123)\n",
      "\tat org.apache.spark.api.python.PythonAccumulatorV2.merge(PythonRDD.scala:747)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$updateAccumulators$1(DAGScheduler.scala:1694)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$updateAccumulators$1$adapted(DAGScheduler.scala:1685)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.updateAccumulators(DAGScheduler.scala:1685)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1838)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3054)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "25/03/20 01:57:39 ERROR DAGScheduler: Failed to update accumulator 0 (org.apache.spark.api.python.PythonAccumulatorV2) for task 0\n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:111)\n",
      "\tat java.net.SocketOutputStream.write(SocketOutputStream.java:155)\n",
      "\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n",
      "\tat java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)\n",
      "\tat java.io.DataOutputStream.flush(DataOutputStream.java:123)\n",
      "\tat org.apache.spark.api.python.PythonAccumulatorV2.merge(PythonRDD.scala:747)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$updateAccumulators$1(DAGScheduler.scala:1694)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$updateAccumulators$1$adapted(DAGScheduler.scala:1685)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.updateAccumulators(DAGScheduler.scala:1685)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1838)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3054)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+---------------+\n",
      "|          subreddit|avg_sentiment_score|sentiment_label|\n",
      "+-------------------+-------------------+---------------+\n",
      "|         offmychest|             0.2412|       positive|\n",
      "|          AskReddit|             0.1281|       positive|\n",
      "|             videos|             0.0718|       positive|\n",
      "|              DotA2|             0.3839|       positive|\n",
      "|      todayilearned|             0.1156|       positive|\n",
      "|      AdviceAnimals|              0.074|       positive|\n",
      "|     DestinyTheGame|              0.377|       positive|\n",
      "|      relationships|             0.4971|       positive|\n",
      "|               pics|             0.0994|       positive|\n",
      "|            Fitness|             0.3721|       positive|\n",
      "|         reddit.com|              0.242|       positive|\n",
      "|          worldnews|            -0.1374|       negative|\n",
      "|    TwoXChromosomes|             0.3153|       positive|\n",
      "|           politics|             0.0821|       positive|\n",
      "|               IAmA|             0.2157|       positive|\n",
      "|              trees|             0.3952|       positive|\n",
      "|relationship_advice|             0.5735|       positive|\n",
      "|    leagueoflegends|             0.3298|       positive|\n",
      "|  explainlikeimfive|             0.2446|       positive|\n",
      "|                WTF|            -0.0267|        neutral|\n",
      "|            atheism|             0.2459|       positive|\n",
      "|             gaming|             0.3748|       positive|\n",
      "|               tifu|             0.1153|       positive|\n",
      "|                sex|             0.5439|       positive|\n",
      "|              funny|             0.0727|       positive|\n",
      "+-------------------+-------------------+---------------+\n",
      "\n",
      "32.5193869349896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/20 01:57:39 ERROR DAGScheduler: Failed to update accumulator 0 (org.apache.spark.api.python.PythonAccumulatorV2) for task 0\n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:111)\n",
      "\tat java.net.SocketOutputStream.write(SocketOutputStream.java:155)\n",
      "\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n",
      "\tat java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)\n",
      "\tat java.io.DataOutputStream.flush(DataOutputStream.java:123)\n",
      "\tat org.apache.spark.api.python.PythonAccumulatorV2.merge(PythonRDD.scala:747)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$updateAccumulators$1(DAGScheduler.scala:1694)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$updateAccumulators$1$adapted(DAGScheduler.scala:1685)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.updateAccumulators(DAGScheduler.scala:1685)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1833)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3054)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "init_time = perf_counter()\n",
    "\n",
    "df_prep = df.drop('body', 'content', 'id', 'subreddit_id', 'title', 'author', 'content_len', 'summary', 'summary_len')\n",
    "\n",
    "k = 25 # number of subreddits to consider\n",
    "df_top = df_prep.select('subreddit') \\\n",
    "                .groupBy('subreddit').count() \\\n",
    "                .sort('count', ascending=False) \\\n",
    "                .limit(k)\n",
    "\n",
    "topk_subreddits = [df_top.collect()[i][0] for i in range(k)]\n",
    "\n",
    "df_topk = df_prep.filter(df_prep['subreddit'].isin(topk_subreddits))\n",
    "\n",
    "df_topk = df_topk.withColumn(\"clean_text\", preprocess_text_udf(df_topk['normalizedBody']))\n",
    "\n",
    "df_vader = df_topk.withColumn(\"sentiment_score\", sentiment_score_udf(df_topk['clean_text']))\n",
    "df_vader = df_vader.withColumn(\"sentiment_label\", sentiment_label_udf(df_vader['sentiment_score']))\n",
    "\n",
    "df_avg = df_vader.groupBy(\"subreddit\") \\\n",
    "                 .agg(F.round(F.avg(\"sentiment_score\"), 4).alias(\"avg_sentiment_score\")) \\\n",
    "                 .limit(k)\n",
    "\n",
    "df_avg = df_avg.withColumn(\"sentiment_label\", sentiment_label_udf(df_avg['avg_sentiment_score']))\n",
    "\n",
    "df_avg.show(k)\n",
    "\n",
    "end_time = perf_counter()\n",
    "\n",
    "elapsed_time = end_time - init_time\n",
    "print(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac8429ac-99e6-4d4c-abdd-b6728c0a8afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_session.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df7a708-8f46-4b83-bca4-f68439f88b7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68c1cd9a-2f58-4fbd-b5dc-89cdb38ccaad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35157ff8-ccb2-4572-bbba-e82bd519c1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_session = SparkSession.builder \\\n",
    "    .master(\"spark://192.168.2.47:7077\") \\\n",
    "    .appName(\"manual_test\") \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", True) \\\n",
    "    .config(\"spark.dynamicAllocation.shuffleTracking.enabled\", True) \\\n",
    "    .config(\"spark.shuffle.service.enabled\", False) \\\n",
    "    .config(\"spark.dynamicAllocation.executorIdleTimeout\", \"30s\") \\\n",
    "    .config(\"spark.executor.instances\", 3) \\\n",
    "    .config(\"spark.executor.memory\", \"5G\") \\\n",
    "    .config(\"spark.cores.max\", 12) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# RDD API\n",
    "spark_context = spark_session.sparkContext\n",
    "spark_context.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c8be6039-df72-4511-bd84-c71b28ffaac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df = spark_session.read.option(\"mode\", \"DROPMALFORMED\").json(\"hdfs://192.168.2.47:9000/data-project/reddit_50k.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a915a485-60bf-42b6-a3fd-1e7c5f9be2fc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/20 01:59:36 ERROR DAGScheduler: Failed to update accumulator 0 (org.apache.spark.api.python.PythonAccumulatorV2) for task 3\n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:111)\n",
      "\tat java.net.SocketOutputStream.write(SocketOutputStream.java:155)\n",
      "\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n",
      "\tat java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)\n",
      "\tat java.io.DataOutputStream.flush(DataOutputStream.java:123)\n",
      "\tat org.apache.spark.api.python.PythonAccumulatorV2.merge(PythonRDD.scala:747)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$updateAccumulators$1(DAGScheduler.scala:1694)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$updateAccumulators$1$adapted(DAGScheduler.scala:1685)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.updateAccumulators(DAGScheduler.scala:1685)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1838)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3054)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "25/03/20 01:59:38 ERROR DAGScheduler: Failed to update accumulator 0 (org.apache.spark.api.python.PythonAccumulatorV2) for task 2\n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:111)\n",
      "\tat java.net.SocketOutputStream.write(SocketOutputStream.java:155)\n",
      "\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n",
      "\tat java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)\n",
      "\tat java.io.DataOutputStream.flush(DataOutputStream.java:123)\n",
      "\tat org.apache.spark.api.python.PythonAccumulatorV2.merge(PythonRDD.scala:747)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$updateAccumulators$1(DAGScheduler.scala:1694)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$updateAccumulators$1$adapted(DAGScheduler.scala:1685)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.updateAccumulators(DAGScheduler.scala:1685)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1838)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3054)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "25/03/20 01:59:38 ERROR DAGScheduler: Failed to update accumulator 0 (org.apache.spark.api.python.PythonAccumulatorV2) for task 0\n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:111)\n",
      "\tat java.net.SocketOutputStream.write(SocketOutputStream.java:155)\n",
      "\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n",
      "\tat java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)\n",
      "\tat java.io.DataOutputStream.flush(DataOutputStream.java:123)\n",
      "\tat org.apache.spark.api.python.PythonAccumulatorV2.merge(PythonRDD.scala:747)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$updateAccumulators$1(DAGScheduler.scala:1694)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$updateAccumulators$1$adapted(DAGScheduler.scala:1685)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.updateAccumulators(DAGScheduler.scala:1685)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1838)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3054)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "25/03/20 01:59:39 ERROR DAGScheduler: Failed to update accumulator 0 (org.apache.spark.api.python.PythonAccumulatorV2) for task 1\n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:111)\n",
      "\tat java.net.SocketOutputStream.write(SocketOutputStream.java:155)\n",
      "\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n",
      "\tat java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)\n",
      "\tat java.io.DataOutputStream.flush(DataOutputStream.java:123)\n",
      "\tat org.apache.spark.api.python.PythonAccumulatorV2.merge(PythonRDD.scala:747)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$updateAccumulators$1(DAGScheduler.scala:1694)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$updateAccumulators$1$adapted(DAGScheduler.scala:1685)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.updateAccumulators(DAGScheduler.scala:1685)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1838)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3054)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------------+---------------+\n",
      "|          subreddit|avg_sentiment_score|sentiment_label|\n",
      "+-------------------+-------------------+---------------+\n",
      "|         offmychest|             0.2412|       positive|\n",
      "|          AskReddit|             0.1281|       positive|\n",
      "|             videos|             0.0718|       positive|\n",
      "|              DotA2|             0.3839|       positive|\n",
      "|      todayilearned|             0.1156|       positive|\n",
      "|      AdviceAnimals|              0.074|       positive|\n",
      "|     DestinyTheGame|              0.377|       positive|\n",
      "|      relationships|             0.4971|       positive|\n",
      "|               pics|             0.0994|       positive|\n",
      "|            Fitness|             0.3721|       positive|\n",
      "|         reddit.com|              0.242|       positive|\n",
      "|          worldnews|            -0.1374|       negative|\n",
      "|    TwoXChromosomes|             0.3153|       positive|\n",
      "|           politics|             0.0821|       positive|\n",
      "|               IAmA|             0.2157|       positive|\n",
      "|              trees|             0.3952|       positive|\n",
      "|relationship_advice|             0.5735|       positive|\n",
      "|    leagueoflegends|             0.3298|       positive|\n",
      "|  explainlikeimfive|             0.2446|       positive|\n",
      "|                WTF|            -0.0267|        neutral|\n",
      "|            atheism|             0.2459|       positive|\n",
      "|             gaming|             0.3748|       positive|\n",
      "|               tifu|             0.1153|       positive|\n",
      "|                sex|             0.5439|       positive|\n",
      "|              funny|             0.0727|       positive|\n",
      "+-------------------+-------------------+---------------+\n",
      "\n",
      "32.33826153798145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/20 01:59:39 ERROR DAGScheduler: Failed to update accumulator 0 (org.apache.spark.api.python.PythonAccumulatorV2) for task 0\n",
      "java.net.SocketException: Broken pipe (Write failed)\n",
      "\tat java.net.SocketOutputStream.socketWrite0(Native Method)\n",
      "\tat java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:111)\n",
      "\tat java.net.SocketOutputStream.write(SocketOutputStream.java:155)\n",
      "\tat java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)\n",
      "\tat java.io.BufferedOutputStream.flush(BufferedOutputStream.java:140)\n",
      "\tat java.io.DataOutputStream.flush(DataOutputStream.java:123)\n",
      "\tat org.apache.spark.api.python.PythonAccumulatorV2.merge(PythonRDD.scala:747)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$updateAccumulators$1(DAGScheduler.scala:1694)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$updateAccumulators$1$adapted(DAGScheduler.scala:1685)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.updateAccumulators(DAGScheduler.scala:1685)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskCompletion(DAGScheduler.scala:1833)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3054)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "init_time = perf_counter()\n",
    "\n",
    "df_prep = df.drop('body', 'content', 'id', 'subreddit_id', 'title', 'author', 'content_len', 'summary', 'summary_len')\n",
    "\n",
    "k = 25 # number of subreddits to consider\n",
    "df_top = df_prep.select('subreddit') \\\n",
    "                .groupBy('subreddit').count() \\\n",
    "                .sort('count', ascending=False) \\\n",
    "                .limit(k)\n",
    "\n",
    "topk_subreddits = [df_top.collect()[i][0] for i in range(k)]\n",
    "\n",
    "df_topk = df_prep.filter(df_prep['subreddit'].isin(topk_subreddits))\n",
    "\n",
    "df_topk = df_topk.withColumn(\"clean_text\", preprocess_text_udf(df_topk['normalizedBody']))\n",
    "\n",
    "df_vader = df_topk.withColumn(\"sentiment_score\", sentiment_score_udf(df_topk['clean_text']))\n",
    "df_vader = df_vader.withColumn(\"sentiment_label\", sentiment_label_udf(df_vader['sentiment_score']))\n",
    "\n",
    "df_avg = df_vader.groupBy(\"subreddit\") \\\n",
    "                 .agg(F.round(F.avg(\"sentiment_score\"), 4).alias(\"avg_sentiment_score\")) \\\n",
    "                 .limit(k)\n",
    "\n",
    "df_avg = df_avg.withColumn(\"sentiment_label\", sentiment_label_udf(df_avg['avg_sentiment_score']))\n",
    "\n",
    "df_avg.show(k)\n",
    "\n",
    "end_time = perf_counter()\n",
    "\n",
    "elapsed_time = end_time - init_time\n",
    "print(elapsed_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72e4a18-f1f8-42d5-a27d-2c688779179b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_session.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
